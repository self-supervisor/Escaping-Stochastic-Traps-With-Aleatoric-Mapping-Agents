{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Uncertainty best hyperparams....\n",
      "icm lr: 0.001\n",
      "reward_weighting: 10.0\n",
      "novel_states_visited: 30.0\n",
      "With Uncertainty best hyperparams...\n",
      "icm lr: 0.0001\n",
      "reward_weighting: 10.0\n",
      "novel_states_visited: 32.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.0e-04, 1.0e+01, 3.2e+01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def get_experiments():\n",
    "    \"\"\"\n",
    "    Gets different experimental settings that \n",
    "    need to be averaged over different seeds.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    experiments : list of str \n",
    "        list of all the experiment types performed \n",
    "    filtered_csvs: list of str \n",
    "        all of the csvs file paths that are relevant\n",
    "        (those not relating to curiosity are removed)  \n",
    "    \"\"\"\n",
    "    all_csvs = glob.glob(\"*.csv\")\n",
    "    filtered_csvs = []\n",
    "    for i, value in enumerate(all_csvs):\n",
    "        if \"curiosity_False\" in value:\n",
    "            continue\n",
    "        filtered_csvs.append(value)\n",
    "    \n",
    "    csvs_without_suffix = [item[:-6] for item in filtered_csvs]\n",
    "    experiments = list(set(csvs_without_suffix))\n",
    "    return experiments, filtered_csvs\n",
    "    \n",
    "def get_all_files_of_same_type(experiments):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    ------- \n",
    "    dict_holding_files_and_run_types: dict\n",
    "        A dictionary of different experiment types as keys \n",
    "        along with the paths to experimental results for that \n",
    "        key across different seeds. \n",
    "    \"\"\"\n",
    "    dict_holding_files_and_run_types = {}\n",
    "    for a_run_type in experiments:\n",
    "        all_files_of_run_type = []\n",
    "        for csv_file in filtered_csvs:\n",
    "            if a_run_type in csv_file:\n",
    "                all_files_of_run_type.append(csv_file)\n",
    "        dict_holding_files_and_run_types[a_run_type] = all_files_of_run_type\n",
    "    return dict_holding_files_and_run_types\n",
    "        \n",
    "def average_over_run_type(run_csvs):\n",
    "    \"\"\"\n",
    "    Computes average performance for different seeds\n",
    "    for a given experimental type.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data_list : numpy.ndarray \n",
    "        mean learning rate, scaling parameter and number\n",
    "        of states visited for a given run experiment type \n",
    "    \"\"\"\n",
    "    data_list = make_run_list(run_csvs)\n",
    "    data_list = np.array(data_list, dtype=np.float64)\n",
    "    data_list = np.mean(data_list, axis=0)\n",
    "    return data_list\n",
    "    \n",
    "def make_run_list(run_csvs):\n",
    "    \"\"\"\n",
    "    Reads list of csvs to give performance of the different\n",
    "    hyperparameter settings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_list : list of numpy.ndarrays \n",
    "        performance of different hyperaparameter settings\n",
    "        for the csvs given as input.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "\n",
    "    data_list = []\n",
    "    \n",
    "    for a_csv in run_csvs:\n",
    "        with open(a_csv, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            data_as_list = list(reader)\n",
    "        f.close()\n",
    "        data_list.append(data_as_list)\n",
    "    return data_list\n",
    "\n",
    "def average_over_multiple_run_types(files_of_same_type, list_of_run_types):\n",
    "    \"\"\"\n",
    "    Gets average performance with and without a noisy TV present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    values: numpy.ndarray\n",
    "        Average performance across seeds as well as with and without a\n",
    "        noisy TV for a given hyperparameter setting.\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for run_type in list_of_run_types:\n",
    "        value = average_over_run_type(files_of_same_type[run_type])\n",
    "        values.append(value)\n",
    "    values = np.array(values)\n",
    "    return np.mean(values, axis=0)\n",
    "\n",
    "def get_best_hyperparam(average_performance_with_and_without_tv):\n",
    "    \"\"\"\n",
    "    Mines best hyperaparam from average performance of various settings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_row : numpy.ndarray \n",
    "        The learning rate, scaling factor and novel states visited of the\n",
    "        best performing hyperaparameters.\n",
    "    \"\"\"\n",
    "    max_val = 0\n",
    "    for row in average_performance_with_and_without_tv:\n",
    "        if row[2] >= max_val:\n",
    "            best_row = row\n",
    "            max_val = row[2]\n",
    "    print(\"icm lr: \" + str(best_row[0]))\n",
    "    print(\"reward_weighting: \" + str(best_row[1]))\n",
    "    print(\"novel_states_visited: \" + str(best_row[2]))\n",
    "    return best_row    \n",
    "\n",
    "experiments, filtered_csvs = get_experiments()\n",
    "print(\"Without Uncertainty best hyperparams....\")\n",
    "files_of_same_type = get_all_files_of_same_type(experiments)\n",
    "average_performance_with_and_without_tv = average_over_multiple_run_types(files_of_same_type, [\"frames_8_noisy_tv_False_curiosity_True_uncertainty_False_random\", \"frames_8_noisy_tv_False_curiosity_True_uncertainty_False_random\"])\n",
    "get_best_hyperparam(average_performance_with_and_without_tv)\n",
    "print(\"With Uncertainty best hyperparams...\")\n",
    "files_of_same_type = get_all_files_of_same_type(experiments)\n",
    "average_performance_with_and_without_tv = average_over_multiple_run_types(files_of_same_type, [\"frames_8_noisy_tv_False_curiosity_True_uncertainty_True_random\", \"frames_8_noisy_tv_False_curiosity_True_uncertainty_True_random\"])\n",
    "get_best_hyperparam(average_performance_with_and_without_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "test_average_over_run_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_all_files_of_same_type():\n",
    "    all_csvs = glob.glob(\"*.csv\")\n",
    "    experiments, _ = get_experiments()\n",
    "    resultant_dict = get_all_files_of_same_type(experiments)\n",
    "    assert len(resultant_dict.items()) == 6\n",
    "    for key, value in resultant_dict.items():\n",
    "        run_types = []\n",
    "        for a_value in value:\n",
    "            run_types.append(get_run_type(a_value))\n",
    "        assert len(set(run_types)) == 1\n",
    "\n",
    "def test_get_experiments():\n",
    "    import re\n",
    "    \n",
    "    def hasNumbers(inputString):\n",
    "        \"\"\"\n",
    "        https://stackoverflow.com/questions/19859282/check-if-a-string-contains-a-number\n",
    "        \"\"\"\n",
    "        return bool(re.search(r'\\d', inputString))\n",
    "    \n",
    "    all_csvs = glob.glob(\"*.csv\")\n",
    "    experiments, filtered_csvs = get_experiments()\n",
    "    \n",
    "    assert all_csvs != filtered_csvs\n",
    "    \n",
    "    # check csvs are filtered to not include curiosity == False\n",
    "    for i, value in filtered_csvs:\n",
    "        if \"curiosity_False\" in value:\n",
    "            raise ValueError\n",
    "    \n",
    "    # check all numbers have been removed from the csv strigns\n",
    "    for i, value in filtered_csvs:\n",
    "        assert hasNumbers(value[:-6]) == False\n",
    "    \n",
    "    # check that all duplicate experiments have been removed\n",
    "    assert len(filtered_csvs) == len(set(filtered_csvs))\n",
    "\n",
    "def get_run_type(csv_file_name):\n",
    "    pass\n",
    "\n",
    "def make_fake_csvs(first_csv_string, second_csv_string):\n",
    "    import csv\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        os.remove(\"fake1.csv\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    try:\n",
    "        os.remove(\"fake2.csv\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    assert len(first_csv_string) == len(second_csv_string)\n",
    "    \n",
    "    for i in range(len(first_csv_string)):\n",
    "        with open(\"fake1.csv\", \"a\") as fp:\n",
    "            wr = csv.writer(fp)\n",
    "            wr.writerow([float(first_csv_string[i][0]), float(first_csv_string[i][1]), float(first_csv_string[i][2])])\n",
    "    \n",
    "    for i in range(len(second_csv_string)):\n",
    "        with open(\"fake2.csv\", \"a\") as fp:\n",
    "            wr = csv.writer(fp)\n",
    "            wr.writerow([float(second_csv_string[i][0]), float(second_csv_string[i][1]), float(second_csv_string[i][2])])\n",
    "        \n",
    "def test_average_over_run_type():\n",
    "    first_csv_string = [[0.01,0.1,13],[0.001,0.1,12],[0.0001,0.1,13], [0.01,1,17], [0.001,1,20], [0.0001,1,18]]\n",
    "    second_csv_string = [[0.01,0.1,17], [0.001,0.1,16], [0.0001,0.1,16], [0.01,1,20], [0.001,1,20], [0.0001,1,21]]\n",
    "    make_fake_csvs(first_csv_string, second_csv_string)\n",
    "    result = average_over_run_type([\"fake1.csv\", \"fake2.csv\"])\n",
    "    manually_computed_result = np.mean(np.array([np.array(first_csv_string), np.array(second_csv_string)]), axis=0)\n",
    "    assert np.array_equal(result, manually_computed_result) == True    \n",
    "\n",
    "def test_make_run_list():\n",
    "    pass\n",
    "\n",
    "def test_average_over_multiple_run_types():\n",
    "    # assert output shape is correct\n",
    "    pass\n",
    "\n",
    "def test_get_best_hyperparam():\n",
    "    fake_average_performance_with_and_without_tv = np.zeros((3, 3))\n",
    "    fake_average_performance_with_and_without_tv[1] = np.ones((3, 1))\n",
    "    assert get_best_hyperparam(fake_average_performance_with_and_without_tv) == fake_average_performance_with_and_without_tv[1] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
